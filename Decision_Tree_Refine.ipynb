{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Numeric Count is 1183747 \n",
      "Training data count is  828549 and Test data count is 354686  \n"
     ]
    }
   ],
   "source": [
    "#Extract Training data\n",
    "raw_numeric = sc.textFile(\"train_numeric.csv.gz\")\n",
    "\n",
    "#Remove header\n",
    "header = raw_numeric.first()\n",
    "raw_numeric_noheader = raw_numeric.filter(lambda line:line != header)\n",
    "\n",
    "#Count the dataset\n",
    "numeric_count = raw_numeric_noheader.count()\n",
    "print \"Total Numeric Count is {} \".format(numeric_count)\n",
    "\n",
    "## Sample Training and trainign data from the cleaned raw RDD!\n",
    "raw_training_sample = raw_numeric_noheader.sample(False, 0.7, 1234)\n",
    "raw_test_sample = raw_numeric_noheader.sample(False, 0.3, 12345)\n",
    "\n",
    "training_count = raw_training_sample.count() \n",
    "test_count = raw_test_sample.count()\n",
    "print\"Training data count is  {} and Test data count is {}  \".format(training_count,test_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.581 percent of products are found to be defective\n",
      "0.585 percent of products  found to be defective in training \n",
      "0.58 percent of products are found to be defective in test set \n"
     ]
    }
   ],
   "source": [
    "#Convert raw data to csv\n",
    "csv_training_data = raw_training_sample.map(lambda x: x.split(\",\"))\n",
    "csv_test_data = raw_test_sample.map(lambda x: x.split(\",\"))\n",
    "csv_total_data = raw_numeric_noheader.map(lambda x: x.split(\",\"))\n",
    "\n",
    "# Analysze the defect part with in total training set \n",
    "id_response_numeric_key_value =  csv_total_data.map(lambda x: ( x[-1], 1 )) \n",
    "defect_numeric_key_value = id_response_numeric_key_value.filter(lambda y: y[0]==\"1\" )\n",
    "defect_counts_by_key = defect_numeric_key_value.countByKey()\n",
    "defect_percent = 100*float(defect_counts_by_key.values()[0])/float(numeric_count)\n",
    "print\"{} percent of products are found to be defective\".format(round(defect_percent,3))\n",
    "\n",
    "# Analysze the defect part with in split training data sets \n",
    "id_response_train_key_value =  csv_training_data.map(lambda x: ( x[-1], 1 )) \n",
    "defect_train_key_value = id_response_train_key_value.filter(lambda y: y[0]==\"1\" )\n",
    "defect_counts_by_key = defect_train_key_value.countByKey()\n",
    "defect_percent = 100*float(defect_counts_by_key.values()[0])/float(training_count)\n",
    "print\"{} percent of products  found to be defective in training \".format(round(defect_percent,3))\n",
    "\n",
    "# Analysze the defect part with in split test data sets \n",
    "id_response_test_key_value =  csv_test_data.map(lambda x: ( x[-1], 1 )) \n",
    "defect_test_key_value = id_response_test_key_value.filter(lambda y: y[0]==\"1\" )\n",
    "defect_counts_by_key = defect_test_key_value.countByKey()\n",
    "defect_percent = 100*float(defect_counts_by_key.values()[0])/float(test_count)\n",
    "print\"{} percent of products are found to be defective in test set \".format(round(defect_percent,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree Classifier trained in 2555.466 seconds\n"
     ]
    }
   ],
   "source": [
    "#A function to Create Labelpoint for training and validation\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from numpy import array\n",
    "from numpy import nan\n",
    "\n",
    "def empty_element_to_dense_lv(line):\n",
    "    nline = line[1:-1]\n",
    "    for i,item in enumerate(nline):\n",
    "        if item == '':\n",
    "            nline[i] = nan\n",
    "    return LabeledPoint(float(line[-1]), array([float(x) for x in nline]))\n",
    "\n",
    "## Create data for Supervised learning \n",
    "training_data = csv_training_data.map(empty_element_to_dense_lv)\n",
    "test_data = csv_test_data.map(empty_element_to_dense_lv)\n",
    "\n",
    "#Train the model \n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from time import time\n",
    "\n",
    "# Build the model\n",
    "t0 = time()\n",
    "tree_model = DecisionTree.trainClassifier(training_data,2,{},'gini',16,120)\n",
    "tt = time() - t0\n",
    "\n",
    "\n",
    "print \"Decision tree Classifier trained in {} seconds\".format(round(tt,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction made in 248.719 seconds. Test accuracy is 0.9942\n",
      "MCC is caluclated in  527.847 seconds. The MCC for Decision tree model is : 0.1197 \n"
     ]
    }
   ],
   "source": [
    "from numpy import sqrt\n",
    "## Predict \n",
    "predictions = tree_model.predict(test_data.map(lambda p: p.features))\n",
    "labels_and_preds = test_data.map(lambda p: p.label).zip(predictions)\n",
    "\n",
    "#Accuracy checks\n",
    "t0 = time()\n",
    "test_accuracy = labels_and_preds.filter(lambda (v, p): v == p).count() / float(test_data.count())\n",
    "tt = time() - t0\n",
    "\n",
    "print \"Prediction made in {} seconds. Test accuracy is {}\".format(round(tt,3), round(test_accuracy,4))\n",
    "\n",
    "#Compute MCC\n",
    "t0 = time()\n",
    "\n",
    "TP = labels_and_preds.filter(lambda (v, p): int(v) == 1 and  int(p) == 1 ).count()\n",
    "FP = labels_and_preds.filter(lambda (v, p): int(v) == 0 and  int(p)  == 1 ).count()\n",
    "FN = labels_and_preds.filter(lambda (v, p): int(v) == 1 and  int(p) == 0 ).count()\n",
    "TN = labels_and_preds.filter(lambda (v, p): int(v) == 0 and  int(p) == 0 ).count()\n",
    "MCC = float(TP*TN - FP*FN)/float(sqrt( (TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)   ) )\n",
    "tt = time() - t0\n",
    "print \"MCC is caluclated in  {} seconds. The MCC for Decision tree model is : {} \".format(round(tt,3), round(MCC,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree_model.save(sc,\"Decision_tree_best_nan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
